{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537e56db",
   "metadata": {},
   "source": [
    "# Plant Pathology 2020 - Traditional Machine Learning Approach\n",
    "\n",
    "## Celebal Technologies Summer Internship 2025 Project\n",
    "\n",
    "**Author:** [Your Name]  \n",
    "**Date:** [Current Date]\n",
    "\n",
    "This notebook implements traditional machine learning approaches for the Plant Pathology 2020 dataset as part of the Celebal Technologies Summer Internship 2025. We'll explore how conventional ML techniques like SVM, Random Forest, and Gradient Boosting compare to deep learning approaches for plant disease classification.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Plant diseases pose a significant threat to food security and agricultural productivity worldwide. Early and accurate diagnosis of plant diseases is crucial for effective disease management. In this notebook, we'll tackle the Plant Pathology 2020 dataset using traditional machine learning approaches, complementing the deep learning approach implemented in the `plant-pathology-2020-resnet50.ipynb` notebook.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "1. Extract meaningful features from plant leaf images\n",
    "2. Train and evaluate various traditional ML models:\n",
    "   - Support Vector Machine (SVM)\n",
    "   - Random Forest\n",
    "   - Gradient Boosting\n",
    "3. Compare performance across models\n",
    "4. Compare traditional ML approaches with deep learning\n",
    "\n",
    "### Dataset Overview\n",
    "\n",
    "The Plant Pathology 2020 dataset contains images of apple leaves with four classes:\n",
    "- Healthy\n",
    "- Multiple Diseases\n",
    "- Rust\n",
    "- Scab\n",
    "\n",
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e062ef20",
   "metadata": {},
   "source": [
    "# ðŸŒ¿ Plant Pathology Classification - Celebal Summer Internship 2025\n",
    "\n",
    "## Traditional Machine Learning Approach with Handcrafted Features\n",
    "\n",
    "This notebook implements the traditional machine learning approach to plant pathology classification as specified in the Celebal Technologies Summer Internship Programme 2025. We use handcrafted features extracted from apple leaf images to identify plant diseases using SVM, Random Forest, and Gradient Boosting Machine classifiers.\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "The goal is to classify apple leaf images into four categories:\n",
    "- ðŸŒ± **Healthy**: Normal leaves without any disease\n",
    "- ðŸ¦  **Multiple Diseases**: Leaves showing symptoms of multiple infections\n",
    "- ðŸ”¶ **Rust**: Leaves with rust disease (orange/brown spots)\n",
    "- ðŸ”´ **Scab**: Leaves with scab disease (dark lesions)\n",
    "\n",
    "In this notebook, we focus on implementing the project's original objective:\n",
    "\n",
    "> \"Aim to classify images into multiple categories, such as identifying different species of plants or animals, using traditional machine learning techniques rather than transfer learning. We will extract handcrafted features from the images and train machine learning models, such as Support Vector Machines (SVM), Random Forests, or Gradient Boosting Machines, to perform the classification task.\"\n",
    "\n",
    "Our implementation will showcase the power of feature engineering and traditional machine learning algorithms for image classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d413ffdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "# Core libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from skimage.feature import graycomatrix, graycoprops, hog\n",
    "from skimage.measure import label, regionprops\n",
    "from scipy import ndimage\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, roc_curve, auc\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489d87fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "\n",
    "CONFIG = {\n",
    "    # Data paths\n",
    "    'DATA_PATH': '../input/plant-pathology-2020-fgvc7',\n",
    "    'IMAGE_PATH': '../input/plant-pathology-2020-fgvc7/images',\n",
    "    \n",
    "    # Image parameters\n",
    "    'IMG_SIZE': (224, 224),  # Size for traditional ML (smaller than DL for faster processing)\n",
    "    'TARGET_COLS': ['healthy', 'multiple_diseases', 'rust', 'scab'],\n",
    "    \n",
    "    # Feature extraction parameters\n",
    "    'COLOR_HIST_BINS': 32,  # Number of bins for color histogram\n",
    "    'HOG_ORIENTATIONS': 8,  # HOG parameters\n",
    "    'HOG_PIXELS_PER_CELL': (16, 16),\n",
    "    'HOG_CELLS_PER_BLOCK': (1, 1),\n",
    "    \n",
    "    # PCA parameters\n",
    "    'USE_PCA': True,\n",
    "    'PCA_COMPONENTS': 200,  # Will be tuned during analysis\n",
    "    \n",
    "    # Training parameters\n",
    "    'VALIDATION_SPLIT': 0.15,\n",
    "    'N_FOLDS': 5,  # For cross-validation\n",
    "    \n",
    "    # Model hyperparameters (initial values, will be tuned)\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.01, 0.1],\n",
    "        'kernel': ['rbf']\n",
    "    },\n",
    "    'RANDOM_FOREST': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    },\n",
    "    'GRADIENT_BOOSTING': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': [3, 5]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create directory structure for saving models if it doesn't exist\n",
    "os.makedirs('ml_models', exist_ok=True)\n",
    "os.makedirs('feature_extractors', exist_ok=True)\n",
    "os.makedirs('visualizations', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a741f471",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration\n",
    "\n",
    "In this section, we'll load the Plant Pathology 2020 dataset, explore the class distribution, and visualize sample images from each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee549dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading function\n",
    "def load_data(config):\n",
    "    \"\"\"Load and preprocess training and test data\"\"\"\n",
    "    try:\n",
    "        # Load CSV files\n",
    "        train = pd.read_csv(f\"{config['DATA_PATH']}/train.csv\")\n",
    "        test = pd.read_csv(f\"{config['DATA_PATH']}/test.csv\")\n",
    "        \n",
    "        # Add .jpg extension to image_id\n",
    "        train['image_id'] = train['image_id'] + '.jpg'\n",
    "        test['image_id'] = test['image_id'] + '.jpg'\n",
    "        \n",
    "        # Create full path for images\n",
    "        train['image_path'] = train['image_id'].apply(lambda x: os.path.join(config['IMAGE_PATH'], x))\n",
    "        test['image_path'] = test['image_id'].apply(lambda x: os.path.join(config['IMAGE_PATH'], x))\n",
    "        \n",
    "        # Analyze class distribution\n",
    "        print(f\"Training data shape: {train.shape}\")\n",
    "        print(f\"Test data shape: {test.shape}\")\n",
    "        print(\"\\nClass distribution in training data:\")\n",
    "        for col in config['TARGET_COLS']:\n",
    "            count = train[col].sum()\n",
    "            percent = count / len(train) * 100\n",
    "            print(f\"{col}: {count} samples ({percent:.2f}%)\")\n",
    "        \n",
    "        return train, test\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Load the data\n",
    "train_df, test_df = load_data(CONFIG)\n",
    "\n",
    "# Display the first few rows of the training data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fb9914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "def plot_class_distribution(df, config):\n",
    "    \"\"\"Visualize class distribution in the dataset\"\"\"\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create counts\n",
    "    counts = [df[col].sum() for col in config['TARGET_COLS']]\n",
    "    percents = [count / len(df) * 100 for count in counts]\n",
    "    \n",
    "    # Plot bar chart with custom colors\n",
    "    colors = ['#2ecc71', '#e74c3c', '#3498db', '#f39c12']\n",
    "    bars = plt.bar(config['TARGET_COLS'], counts, color=colors)\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for bar, count, percent in zip(bars, counts, percents):\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width()/2,\n",
    "            bar.get_height() + 5,\n",
    "            f\"{count}\\n({percent:.1f}%)\",\n",
    "            ha='center',\n",
    "            fontweight='bold'\n",
    "        )\n",
    "    \n",
    "    plt.title('Class Distribution in Training Data', fontsize=16)\n",
    "    plt.ylabel('Number of Samples', fontsize=14)\n",
    "    plt.xlabel('Class', fontsize=14)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot class distribution\n",
    "plot_class_distribution(train_df, CONFIG)\n",
    "\n",
    "# Check for class imbalance\n",
    "print(\"\\nClass balance analysis:\")\n",
    "counts = [train_df[col].sum() for col in CONFIG['TARGET_COLS']]\n",
    "min_count = min(counts)\n",
    "max_count = max(counts)\n",
    "imbalance_ratio = max_count / min_count\n",
    "print(f\"Imbalance ratio (max/min): {imbalance_ratio:.2f}\")\n",
    "\n",
    "if imbalance_ratio > 1.5:\n",
    "    print(\"Note: There is class imbalance in the data. Consider using class weights or balanced sampling.\")\n",
    "else:\n",
    "    print(\"Note: Class distribution is relatively balanced.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de2d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and display sample images\n",
    "def visualize_samples(df, config, num_samples=3):\n",
    "    \"\"\"Visualize sample images from each class\"\"\"\n",
    "    # Create a figure\n",
    "    fig, axes = plt.subplots(len(config['TARGET_COLS']), num_samples, figsize=(15, 12))\n",
    "    \n",
    "    # Plot samples from each class\n",
    "    for i, col in enumerate(config['TARGET_COLS']):\n",
    "        # Get samples from this class\n",
    "        class_samples = df[df[col] == 1]['image_path'].values[:num_samples]\n",
    "        \n",
    "        for j, img_path in enumerate(class_samples):\n",
    "            # Load and resize image\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "            \n",
    "            # Display image\n",
    "            axes[i, j].imshow(img)\n",
    "            axes[i, j].set_title(f\"{col.replace('_', ' ').title()}\", fontsize=12)\n",
    "            axes[i, j].axis('off')\n",
    "    \n",
    "    plt.suptitle('Sample Images from Each Class', fontsize=16, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize sample images\n",
    "visualize_samples(train_df, CONFIG, num_samples=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa64162d",
   "metadata": {},
   "source": [
    "## Image Preprocessing and Feature Extraction\n",
    "\n",
    "In this section, we'll create functions to preprocess the images and extract handcrafted features for our traditional machine learning models. These features will include:\n",
    "\n",
    "1. **Color features**: RGB and HSV color histograms\n",
    "2. **Texture features**: GLCM (Gray-Level Co-Occurrence Matrix) features, Haralick texture features\n",
    "3. **Shape features**: Hu Moments, contour features\n",
    "4. **Edge features**: HOG (Histogram of Oriented Gradients)\n",
    "\n",
    "After extraction, we'll apply dimensionality reduction using PCA to reduce the feature space to a manageable size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb98ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing functions\n",
    "def preprocess_image(image_path, target_size):\n",
    "    \"\"\"\n",
    "    Load and preprocess an image for feature extraction\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image\n",
    "        target_size: Target size as tuple (width, height)\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed image in BGR format\n",
    "    \"\"\"\n",
    "    # Read image\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not read image: {image_path}\")\n",
    "    \n",
    "    # Resize\n",
    "    img_resized = cv2.resize(img, target_size)\n",
    "    \n",
    "    return img_resized\n",
    "\n",
    "# Test the preprocessing function\n",
    "sample_img_path = train_df.iloc[0]['image_path']\n",
    "sample_img = preprocess_image(sample_img_path, CONFIG['IMG_SIZE'])\n",
    "\n",
    "# Display original and preprocessed image\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Original image\n",
    "plt.subplot(1, 2, 1)\n",
    "orig_img = cv2.imread(sample_img_path)\n",
    "orig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(orig_img)\n",
    "plt.title(f\"Original ({orig_img.shape[1]}x{orig_img.shape[0]})\", fontsize=12)\n",
    "plt.axis('off')\n",
    "\n",
    "# Preprocessed image\n",
    "plt.subplot(1, 2, 2)\n",
    "proc_img = cv2.cvtColor(sample_img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(proc_img)\n",
    "plt.title(f\"Preprocessed ({proc_img.shape[1]}x{proc_img.shape[0]})\", fontsize=12)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.suptitle('Image Preprocessing Example', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7409598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction functions\n",
    "\n",
    "def extract_color_features(image):\n",
    "    \"\"\"\n",
    "    Extract color features from an image\n",
    "    \n",
    "    Args:\n",
    "        image: BGR image\n",
    "        \n",
    "    Returns:\n",
    "        Color features array\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # RGB histograms\n",
    "    for channel in range(3):  # BGR channels\n",
    "        histogram = cv2.calcHist([image], [channel], None, [CONFIG['COLOR_HIST_BINS']], [0, 256])\n",
    "        # Normalize histogram\n",
    "        histogram = cv2.normalize(histogram, histogram).flatten()\n",
    "        features.extend(histogram)\n",
    "    \n",
    "    # Convert to HSV and extract histograms\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    for channel in range(3):  # HSV channels\n",
    "        histogram = cv2.calcHist([hsv_image], [channel], None, [CONFIG['COLOR_HIST_BINS']], \n",
    "                                [0, 180] if channel == 0 else [0, 256])  # Hue has range 0-180 in OpenCV\n",
    "        histogram = cv2.normalize(histogram, histogram).flatten()\n",
    "        features.extend(histogram)\n",
    "    \n",
    "    # Mean and std of each channel (BGR)\n",
    "    for channel in range(3):\n",
    "        features.append(np.mean(image[:, :, channel]))\n",
    "        features.append(np.std(image[:, :, channel]))\n",
    "    \n",
    "    # Mean and std of each channel (HSV)\n",
    "    for channel in range(3):\n",
    "        features.append(np.mean(hsv_image[:, :, channel]))\n",
    "        features.append(np.std(hsv_image[:, :, channel]))\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "def extract_texture_features(image):\n",
    "    \"\"\"\n",
    "    Extract texture features from an image using GLCM\n",
    "    \n",
    "    Args:\n",
    "        image: BGR image\n",
    "        \n",
    "    Returns:\n",
    "        Texture features array\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # GLCM properties\n",
    "    distances = [1, 3, 5]\n",
    "    angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "    properties = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM']\n",
    "    \n",
    "    for distance in distances:\n",
    "        glcm = graycomatrix(gray, [distance], angles, 256, symmetric=True, normed=True)\n",
    "        \n",
    "        for prop in properties:\n",
    "            feature = graycoprops(glcm, prop).flatten()\n",
    "            features.extend(feature)\n",
    "    \n",
    "    # Add more texture features (LBP could be added here)\n",
    "    \n",
    "    # Simple texture metrics\n",
    "    features.append(np.mean(gray))  # Mean intensity\n",
    "    features.append(np.std(gray))   # Standard deviation\n",
    "    features.append(np.var(gray))   # Variance\n",
    "    \n",
    "    # Entropy (measure of randomness)\n",
    "    hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n",
    "    hist = hist / hist.sum()\n",
    "    entropy = -np.sum(hist * np.log2(hist + 1e-7))\n",
    "    features.append(entropy)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "def extract_shape_features(image):\n",
    "    \"\"\"\n",
    "    Extract shape features from an image\n",
    "    \n",
    "    Args:\n",
    "        image: BGR image\n",
    "        \n",
    "    Returns:\n",
    "        Shape features array\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Threshold to get binary image\n",
    "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Hu Moments\n",
    "    moments = cv2.moments(binary)\n",
    "    hu_moments = cv2.HuMoments(moments).flatten()\n",
    "    # Log transform to reduce range\n",
    "    hu_moments = -np.sign(hu_moments) * np.log10(np.abs(hu_moments) + 1e-7)\n",
    "    features.extend(hu_moments)\n",
    "    \n",
    "    # Basic shape features from moments\n",
    "    if moments['m00'] != 0:\n",
    "        features.append(moments['m10'] / moments['m00'])  # center of mass X\n",
    "        features.append(moments['m01'] / moments['m00'])  # center of mass Y\n",
    "    else:\n",
    "        features.extend([0, 0])\n",
    "    \n",
    "    # Contours\n",
    "    try:\n",
    "        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if contours:\n",
    "            # Use largest contour\n",
    "            largest_contour = max(contours, key=cv2.contourArea)\n",
    "            \n",
    "            # Area and perimeter\n",
    "            area = cv2.contourArea(largest_contour)\n",
    "            perimeter = cv2.arcLength(largest_contour, True)\n",
    "            features.append(area)\n",
    "            features.append(perimeter)\n",
    "            \n",
    "            # Circularity: 4*Ï€*area/perimeter^2 (1 for perfect circle)\n",
    "            if perimeter > 0:\n",
    "                circularity = 4 * np.pi * area / (perimeter * perimeter)\n",
    "                features.append(circularity)\n",
    "            else:\n",
    "                features.append(0)\n",
    "            \n",
    "            # Bounding rectangle\n",
    "            x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "            features.append(w)\n",
    "            features.append(h)\n",
    "            features.append(w/h)  # aspect ratio\n",
    "            \n",
    "            # Minimum enclosing circle\n",
    "            (x, y), radius = cv2.minEnclosingCircle(largest_contour)\n",
    "            features.append(radius)\n",
    "            \n",
    "            # Convex hull\n",
    "            hull = cv2.convexHull(largest_contour)\n",
    "            hull_area = cv2.contourArea(hull)\n",
    "            if hull_area > 0:\n",
    "                solidity = float(area) / hull_area\n",
    "                features.append(solidity)\n",
    "            else:\n",
    "                features.append(0)\n",
    "        else:\n",
    "            # If no contour found, add zeros\n",
    "            features.extend([0] * 9)  # 9 shape features above\n",
    "    except:\n",
    "        # If contour processing fails, add zeros\n",
    "        features.extend([0] * 9)\n",
    "        \n",
    "    return np.array(features)\n",
    "\n",
    "def extract_hog_features(image):\n",
    "    \"\"\"\n",
    "    Extract HOG (Histogram of Oriented Gradients) features\n",
    "    \n",
    "    Args:\n",
    "        image: BGR image\n",
    "        \n",
    "    Returns:\n",
    "        HOG features array\n",
    "    \"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Extract HOG features\n",
    "    hog_features = hog(\n",
    "        gray, \n",
    "        orientations=CONFIG['HOG_ORIENTATIONS'],\n",
    "        pixels_per_cell=CONFIG['HOG_PIXELS_PER_CELL'],\n",
    "        cells_per_block=CONFIG['HOG_CELLS_PER_BLOCK'],\n",
    "        visualize=False,\n",
    "        block_norm='L2-Hys'\n",
    "    )\n",
    "    \n",
    "    return hog_features\n",
    "\n",
    "def extract_all_features(image):\n",
    "    \"\"\"\n",
    "    Extract all features from an image\n",
    "    \n",
    "    Args:\n",
    "        image: BGR image\n",
    "        \n",
    "    Returns:\n",
    "        All features combined as an array\n",
    "    \"\"\"\n",
    "    color_features = extract_color_features(image)\n",
    "    texture_features = extract_texture_features(image)\n",
    "    shape_features = extract_shape_features(image)\n",
    "    hog_features = extract_hog_features(image)\n",
    "    \n",
    "    # Combine all features\n",
    "    all_features = np.concatenate([color_features, texture_features, shape_features, hog_features])\n",
    "    \n",
    "    return all_features\n",
    "\n",
    "# Test feature extraction on sample image\n",
    "sample_features = extract_all_features(sample_img)\n",
    "\n",
    "print(f\"Total extracted features: {len(sample_features)}\")\n",
    "print(f\"  - Color features: {len(extract_color_features(sample_img))}\")\n",
    "print(f\"  - Texture features: {len(extract_texture_features(sample_img))}\")\n",
    "print(f\"  - Shape features: {len(extract_shape_features(sample_img))}\")\n",
    "print(f\"  - HOG features: {len(extract_hog_features(sample_img))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa44181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for all images\n",
    "def extract_features_from_dataset(df, config, subset=None):\n",
    "    \"\"\"\n",
    "    Extract features from all images in the dataframe\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with image paths\n",
    "        config: Configuration dictionary\n",
    "        subset: Number of images to process (None for all)\n",
    "    \n",
    "    Returns:\n",
    "        X: Features array\n",
    "        y: Labels array (one-hot encoded)\n",
    "    \"\"\"\n",
    "    # Subsample if requested\n",
    "    if subset is not None:\n",
    "        df = df.sample(min(subset, len(df)), random_state=42)\n",
    "    \n",
    "    # Initialize arrays\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # Extract features for each image\n",
    "    print(\"Extracting features from images...\")\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            # Load and preprocess image\n",
    "            image = preprocess_image(row['image_path'], config['IMG_SIZE'])\n",
    "            \n",
    "            # Extract features\n",
    "            features = extract_all_features(image)\n",
    "            \n",
    "            # Add to arrays\n",
    "            X.append(features)\n",
    "            \n",
    "            # Get label (one-hot encoded already in dataframe)\n",
    "            label = row[config['TARGET_COLS']].values\n",
    "            y.append(label)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {row['image_path']}: {e}\")\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    print(f\"Features extracted: {X.shape}, Labels: {y.shape}\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Extract a small sample first to test the pipeline\n",
    "X_sample, y_sample = extract_features_from_dataset(train_df, CONFIG, subset=20)\n",
    "\n",
    "print(\"\\nFeature statistics:\")\n",
    "print(f\"Feature array shape: {X_sample.shape}\")\n",
    "print(f\"Min value: {X_sample.min()}\")\n",
    "print(f\"Max value: {X_sample.max()}\")\n",
    "print(f\"Mean: {X_sample.mean()}\")\n",
    "print(f\"Std: {X_sample.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffecc5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature normalization and dimensionality reduction\n",
    "def normalize_and_reduce_features(X_train, X_val=None, n_components=None):\n",
    "    \"\"\"\n",
    "    Normalize features and apply PCA for dimensionality reduction\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features array\n",
    "        X_val: Validation features array (optional)\n",
    "        n_components: Number of PCA components (None for no PCA)\n",
    "    \n",
    "    Returns:\n",
    "        X_train_norm: Normalized and reduced training features\n",
    "        X_val_norm: Normalized and reduced validation features (if provided)\n",
    "        scaler: Fitted StandardScaler\n",
    "        pca: Fitted PCA (if used)\n",
    "    \"\"\"\n",
    "    # Initialize scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit on training data and transform\n",
    "    X_train_norm = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # Transform validation data if provided\n",
    "    X_val_norm = None\n",
    "    if X_val is not None:\n",
    "        X_val_norm = scaler.transform(X_val)\n",
    "    \n",
    "    # Apply PCA if requested\n",
    "    pca = None\n",
    "    if n_components is not None:\n",
    "        pca = PCA(n_components=n_components, random_state=42)\n",
    "        X_train_norm = pca.fit_transform(X_train_norm)\n",
    "        \n",
    "        # Print explained variance\n",
    "        explained_variance = pca.explained_variance_ratio_.sum()\n",
    "        print(f\"PCA: {n_components} components explain {explained_variance:.2%} of variance\")\n",
    "        \n",
    "        # Transform validation data if provided\n",
    "        if X_val is not None:\n",
    "            X_val_norm = pca.transform(X_val_norm)\n",
    "    \n",
    "    return X_train_norm, X_val_norm, scaler, pca\n",
    "\n",
    "# Apply feature normalization to our sample\n",
    "X_sample_norm, _, scaler_sample, _ = normalize_and_reduce_features(X_sample)\n",
    "\n",
    "print(\"\\nNormalized feature statistics:\")\n",
    "print(f\"Shape: {X_sample_norm.shape}\")\n",
    "print(f\"Min value: {X_sample_norm.min()}\")\n",
    "print(f\"Max value: {X_sample_norm.max()}\")\n",
    "print(f\"Mean: {X_sample_norm.mean()}\")\n",
    "print(f\"Std: {X_sample_norm.std()}\")\n",
    "\n",
    "# Now try with PCA\n",
    "X_sample_norm_pca, _, _, pca_sample = normalize_and_reduce_features(X_sample, n_components=10)\n",
    "\n",
    "print(\"\\nPCA reduced feature statistics:\")\n",
    "print(f\"Shape: {X_sample_norm_pca.shape}\")\n",
    "print(f\"Explained variance ratio per component:\")\n",
    "for i, ratio in enumerate(pca_sample.explained_variance_ratio_):\n",
    "    print(f\"  Component {i+1}: {ratio:.4f} ({ratio*100:.2f}%)\")\n",
    "    \n",
    "# Visualize PCA components\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, len(pca_sample.explained_variance_ratio_) + 1), \n",
    "        pca_sample.explained_variance_ratio_)\n",
    "plt.xlabel('PCA Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance by PCA Component')\n",
    "plt.xticks(range(1, len(pca_sample.explained_variance_ratio_) + 1))\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Determine optimal number of components for the full dataset\n",
    "def find_optimal_pca_components(X, variance_threshold=0.95):\n",
    "    \"\"\"Find optimal number of PCA components to retain specified variance\"\"\"\n",
    "    # Normalize data\n",
    "    scaler = StandardScaler()\n",
    "    X_norm = scaler.fit_transform(X)\n",
    "    \n",
    "    # Apply PCA with all components\n",
    "    pca = PCA().fit(X_norm)\n",
    "    \n",
    "    # Calculate cumulative explained variance\n",
    "    cum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    # Find minimum components needed for threshold\n",
    "    n_components = np.argmax(cum_var >= variance_threshold) + 1\n",
    "    \n",
    "    return n_components, cum_var\n",
    "\n",
    "# Find optimal number of components for our sample\n",
    "n_optimal, cum_var = find_optimal_pca_components(X_sample)\n",
    "\n",
    "print(f\"\\nOptimal number of PCA components for 95% variance: {n_optimal}\")\n",
    "\n",
    "# Plot cumulative explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(cum_var) + 1), cum_var, marker='o')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% Variance Threshold')\n",
    "plt.axvline(x=n_optimal, color='g', linestyle='--', \n",
    "            label=f'Optimal Components: {n_optimal}')\n",
    "plt.xlabel('Number of PCA Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('PCA Explained Variance')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Update CONFIG with optimal PCA components for full training\n",
    "CONFIG['PCA_COMPONENTS'] = n_optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1329919",
   "metadata": {},
   "source": [
    "## Traditional ML Model Implementation\n",
    "\n",
    "Now we'll implement the traditional machine learning models as specified in the project objectives:\n",
    "\n",
    "1. Support Vector Machine (SVM)\n",
    "2. Random Forest\n",
    "3. Gradient Boosting Machine\n",
    "\n",
    "For each model, we'll:\n",
    "- Split the data into training and validation sets\n",
    "- Perform hyperparameter tuning using cross-validation\n",
    "- Evaluate performance on validation data\n",
    "- Analyze feature importance (for tree-based models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56882987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation and train-test split\n",
    "def prepare_data_for_ml_models(config, extract_all=True):\n",
    "    \"\"\"\n",
    "    Prepare data for machine learning models\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dictionary\n",
    "        extract_all: Whether to extract features for all images or use a subset\n",
    "    \n",
    "    Returns:\n",
    "        X_train: Training features\n",
    "        X_val: Validation features\n",
    "        y_train: Training labels\n",
    "        y_val: Validation labels\n",
    "        y_train_binary: Training labels as binary for each class\n",
    "        y_val_binary: Validation labels as binary for each class\n",
    "    \"\"\"\n",
    "    print(\"Loading and preparing data for machine learning models...\")\n",
    "    \n",
    "    # Load data if not already loaded\n",
    "    global train_df\n",
    "    if 'train_df' not in globals() or train_df is None:\n",
    "        train_df, _ = load_data(config)\n",
    "    \n",
    "    # Extract features (use all data if extract_all is True, otherwise sample)\n",
    "    subset = None if extract_all else 100  # Small subset for testing\n",
    "    X, y = extract_features_from_dataset(train_df, config, subset=subset)\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, \n",
    "        test_size=config['VALIDATION_SPLIT'],\n",
    "        random_state=42,\n",
    "        stratify=np.argmax(y, axis=1)  # Stratify based on class\n",
    "    )\n",
    "    \n",
    "    # Apply normalization and PCA\n",
    "    X_train_norm, X_val_norm, scaler, pca = normalize_and_reduce_features(\n",
    "        X_train, X_val, \n",
    "        n_components=config['PCA_COMPONENTS'] if config['USE_PCA'] else None\n",
    "    )\n",
    "    \n",
    "    # Save preprocessing objects for later use with test data\n",
    "    if extract_all:\n",
    "        with open('feature_extractors/scaler.pkl', 'wb') as f:\n",
    "            pickle.dump(scaler, f)\n",
    "        \n",
    "        if config['USE_PCA']:\n",
    "            with open('feature_extractors/pca.pkl', 'wb') as f:\n",
    "                pickle.dump(pca, f)\n",
    "    \n",
    "    # Create binary labels for each class (for evaluation metrics)\n",
    "    y_train_binary = {}\n",
    "    y_val_binary = {}\n",
    "    \n",
    "    for i, col in enumerate(config['TARGET_COLS']):\n",
    "        y_train_binary[col] = y_train[:, i]\n",
    "        y_val_binary[col] = y_val[:, i]\n",
    "    \n",
    "    return X_train_norm, X_val_norm, y_train, y_val, y_train_binary, y_val_binary\n",
    "\n",
    "# Use a small subset of data for testing the pipeline\n",
    "X_train, X_val, y_train, y_val, y_train_binary, y_val_binary = prepare_data_for_ml_models(\n",
    "    CONFIG, extract_all=False\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining data: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation data: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Class distribution in training set:\")\n",
    "for i, col in enumerate(CONFIG['TARGET_COLS']):\n",
    "    print(f\"  {col}: {y_train[:, i].sum()} samples ({y_train[:, i].sum() / len(y_train) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6645ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for model training and evaluation\n",
    "def evaluate_model(model, X_val, y_val, y_val_binary, model_name, config):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        X_val: Validation features\n",
    "        y_val: Validation labels (one-hot encoded)\n",
    "        y_val_binary: Validation labels as binary for each class\n",
    "        model_name: Name of the model for printing\n",
    "        config: Configuration dictionary\n",
    "    \n",
    "    Returns:\n",
    "        metrics: Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    y_pred_proba = model.predict_proba(X_val)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    \n",
    "    # Convert one-hot encoded y_val to class indices\n",
    "    y_true = np.argmax(y_val, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, average='weighted'),\n",
    "        'recall': recall_score(y_true, y_pred, average='weighted'),\n",
    "        'f1': f1_score(y_true, y_pred, average='weighted')\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{model_name} Evaluation Results:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, \n",
    "                                target_names=config['TARGET_COLS'], \n",
    "                                zero_division=0))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=config['TARGET_COLS'],\n",
    "                yticklabels=config['TARGET_COLS'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ROC curves for each class (one-vs-rest)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, class_name in enumerate(config['TARGET_COLS']):\n",
    "        # For multiclass, we need to get the probabilities for the current class\n",
    "        y_score = y_pred_proba[:, i]\n",
    "        fpr, tpr, _ = roc_curve(y_val_binary[class_name], y_score)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.plot(fpr, tpr, lw=2, \n",
    "                 label=f'{class_name} (AUC = {roc_auc:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{model_name} ROC Curves')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0378de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Model Implementation\n",
    "def train_svm_model(X_train, y_train, X_val, y_val, y_val_binary, config):\n",
    "    \"\"\"\n",
    "    Train and evaluate an SVM model\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training labels\n",
    "        X_val: Validation features\n",
    "        y_val: Validation labels\n",
    "        y_val_binary: Validation binary labels for each class\n",
    "        config: Configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        model: Trained SVM model\n",
    "        metrics: Evaluation metrics\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training SVM Model\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Convert one-hot encoded labels to class indices for training\n",
    "    y_train_indices = np.argmax(y_train, axis=1)\n",
    "    \n",
    "    # Create and train SVM model\n",
    "    model = OneVsRestClassifier(SVC(probability=True, random_state=42))\n",
    "    \n",
    "    # Use a subset of parameters for the small test to save time\n",
    "    if X_train.shape[0] <= 100:  # Small test\n",
    "        param_grid = {'estimator__C': [1], 'estimator__gamma': ['scale']}\n",
    "        cv = 2\n",
    "    else:  # Full training\n",
    "        param_grid = config['SVM']\n",
    "        cv = config['N_FOLDS']\n",
    "    \n",
    "    # Grid search for hyperparameter tuning\n",
    "    grid_search = GridSearchCV(\n",
    "        model, param_grid, cv=cv, scoring='accuracy', n_jobs=-1, verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train_indices)\n",
    "    \n",
    "    # Get best model and parameters\n",
    "    model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    # Training time\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nBest Parameters: {best_params}\")\n",
    "    print(f\"Training time: {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = evaluate_model(model, X_val, y_val, y_val_binary, \"SVM\", config)\n",
    "    metrics['training_time'] = training_time\n",
    "    \n",
    "    # Save model if using full dataset\n",
    "    if X_train.shape[0] > 100:\n",
    "        with open('ml_models/svm_model.pkl', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "    \n",
    "    return model, metrics\n",
    "\n",
    "# Train SVM model on our small test set\n",
    "svm_model, svm_metrics = train_svm_model(\n",
    "    X_train, y_train, X_val, y_val, y_val_binary, CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352293e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Model Implementation\n",
    "def train_random_forest_model(X_train, y_train, X_val, y_val, y_val_binary, config):\n",
    "    \"\"\"\n",
    "    Train and evaluate a Random Forest model\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training labels\n",
    "        X_val: Validation features\n",
    "        y_val: Validation labels\n",
    "        y_val_binary: Validation binary labels for each class\n",
    "        config: Configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        model: Trained Random Forest model\n",
    "        metrics: Evaluation metrics\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training Random Forest Model\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Convert one-hot encoded labels to class indices for training\n",
    "    y_train_indices = np.argmax(y_train, axis=1)\n",
    "    \n",
    "    # Create Random Forest model\n",
    "    model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    \n",
    "    # Use a subset of parameters for the small test to save time\n",
    "    if X_train.shape[0] <= 100:  # Small test\n",
    "        param_grid = {'n_estimators': [10], 'max_depth': [5]}\n",
    "        cv = 2\n",
    "    else:  # Full training\n",
    "        param_grid = config['RANDOM_FOREST']\n",
    "        cv = config['N_FOLDS']\n",
    "    \n",
    "    # Grid search for hyperparameter tuning\n",
    "    grid_search = GridSearchCV(\n",
    "        model, param_grid, cv=cv, scoring='accuracy', n_jobs=-1, verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train_indices)\n",
    "    \n",
    "    # Get best model and parameters\n",
    "    model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    # Training time\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nBest Parameters: {best_params}\")\n",
    "    print(f\"Training time: {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = evaluate_model(model, X_val, y_val, y_val_binary, \"Random Forest\", config)\n",
    "    metrics['training_time'] = training_time\n",
    "    \n",
    "    # Feature importance\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Get feature importances\n",
    "        importances = model.feature_importances_\n",
    "        \n",
    "        # Get indices of top 30 features\n",
    "        if len(importances) > 30:\n",
    "            top_indices = np.argsort(importances)[-30:]\n",
    "            plt.title('Top 30 Feature Importances (Random Forest)', fontsize=14)\n",
    "        else:\n",
    "            top_indices = np.argsort(importances)\n",
    "            plt.title('Feature Importances (Random Forest)', fontsize=14)\n",
    "            \n",
    "        # Plot feature importances\n",
    "        plt.barh(range(len(top_indices)), importances[top_indices])\n",
    "        plt.yticks(range(len(top_indices)), [f\"Feature {i}\" for i in top_indices])\n",
    "        plt.xlabel('Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Save model if using full dataset\n",
    "    if X_train.shape[0] > 100:\n",
    "        with open('ml_models/random_forest_model.pkl', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "    \n",
    "    return model, metrics\n",
    "\n",
    "# Train Random Forest model on our small test set\n",
    "rf_model, rf_metrics = train_random_forest_model(\n",
    "    X_train, y_train, X_val, y_val, y_val_binary, CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787e0937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Model Implementation\n",
    "def train_gradient_boosting_model(X_train, y_train, X_val, y_val, y_val_binary, config):\n",
    "    \"\"\"\n",
    "    Train and evaluate a Gradient Boosting model\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training labels\n",
    "        X_val: Validation features\n",
    "        y_val: Validation labels\n",
    "        y_val_binary: Validation binary labels for each class\n",
    "        config: Configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        model: Trained Gradient Boosting model\n",
    "        metrics: Evaluation metrics\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training Gradient Boosting Model\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Convert one-hot encoded labels to class indices for training\n",
    "    y_train_indices = np.argmax(y_train, axis=1)\n",
    "    \n",
    "    # Create Gradient Boosting model\n",
    "    model = GradientBoostingClassifier(random_state=42)\n",
    "    \n",
    "    # Use a subset of parameters for the small test to save time\n",
    "    if X_train.shape[0] <= 100:  # Small test\n",
    "        param_grid = {'n_estimators': [10], 'learning_rate': [0.1]}\n",
    "        cv = 2\n",
    "    else:  # Full training\n",
    "        param_grid = config['GRADIENT_BOOSTING']\n",
    "        cv = config['N_FOLDS']\n",
    "    \n",
    "    # Grid search for hyperparameter tuning\n",
    "    grid_search = GridSearchCV(\n",
    "        model, param_grid, cv=cv, scoring='accuracy', n_jobs=-1, verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train_indices)\n",
    "    \n",
    "    # Get best model and parameters\n",
    "    model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    # Training time\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nBest Parameters: {best_params}\")\n",
    "    print(f\"Training time: {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = evaluate_model(model, X_val, y_val, y_val_binary, \"Gradient Boosting\", config)\n",
    "    metrics['training_time'] = training_time\n",
    "    \n",
    "    # Feature importance\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Get feature importances\n",
    "        importances = model.feature_importances_\n",
    "        \n",
    "        # Get indices of top 30 features\n",
    "        if len(importances) > 30:\n",
    "            top_indices = np.argsort(importances)[-30:]\n",
    "            plt.title('Top 30 Feature Importances (Gradient Boosting)', fontsize=14)\n",
    "        else:\n",
    "            top_indices = np.argsort(importances)\n",
    "            plt.title('Feature Importances (Gradient Boosting)', fontsize=14)\n",
    "            \n",
    "        # Plot feature importances\n",
    "        plt.barh(range(len(top_indices)), importances[top_indices])\n",
    "        plt.yticks(range(len(top_indices)), [f\"Feature {i}\" for i in top_indices])\n",
    "        plt.xlabel('Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Save model if using full dataset\n",
    "    if X_train.shape[0] > 100:\n",
    "        with open('ml_models/gradient_boosting_model.pkl', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "    \n",
    "    return model, metrics\n",
    "\n",
    "# Train Gradient Boosting model on our small test set\n",
    "gb_model, gb_metrics = train_gradient_boosting_model(\n",
    "    X_train, y_train, X_val, y_val, y_val_binary, CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a0089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison\n",
    "def compare_models(models_metrics):\n",
    "    \"\"\"\n",
    "    Compare different models based on their metrics\n",
    "    \n",
    "    Args:\n",
    "        models_metrics: Dictionary of model metrics\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Model Comparison\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Extract metrics for comparison\n",
    "    model_names = []\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "    training_times = []\n",
    "    inference_times = []\n",
    "    \n",
    "    for model_name, metrics in models_metrics.items():\n",
    "        model_names.append(model_name)\n",
    "        accuracies.append(metrics['accuracy'])\n",
    "        f1_scores.append(metrics['f1_score'])\n",
    "        training_times.append(metrics['training_time'])\n",
    "        inference_times.append(metrics['inference_time'])\n",
    "    \n",
    "    # Create comparison dataframe\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Model': model_names,\n",
    "        'Accuracy': accuracies,\n",
    "        'F1 Score': f1_scores,\n",
    "        'Training Time (s)': training_times,\n",
    "        'Inference Time (s)': inference_times\n",
    "    })\n",
    "    \n",
    "    # Sort by accuracy\n",
    "    comparison_df = comparison_df.sort_values('Accuracy', ascending=False)\n",
    "    \n",
    "    # Display comparison table\n",
    "    print(\"\\nModel Comparison Table:\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    axes[0, 0].bar(model_names, accuracies)\n",
    "    axes[0, 0].set_title('Accuracy Comparison', fontsize=14)\n",
    "    axes[0, 0].set_ylabel('Accuracy')\n",
    "    axes[0, 0].set_ylim([0, 1])\n",
    "    for i, v in enumerate(accuracies):\n",
    "        axes[0, 0].text(i, v + 0.02, f\"{v:.3f}\", ha='center')\n",
    "    \n",
    "    # F1 Score comparison\n",
    "    axes[0, 1].bar(model_names, f1_scores)\n",
    "    axes[0, 1].set_title('F1 Score Comparison', fontsize=14)\n",
    "    axes[0, 1].set_ylabel('F1 Score')\n",
    "    axes[0, 1].set_ylim([0, 1])\n",
    "    for i, v in enumerate(f1_scores):\n",
    "        axes[0, 1].text(i, v + 0.02, f\"{v:.3f}\", ha='center')\n",
    "    \n",
    "    # Training time comparison\n",
    "    axes[1, 0].bar(model_names, training_times)\n",
    "    axes[1, 0].set_title('Training Time Comparison', fontsize=14)\n",
    "    axes[1, 0].set_ylabel('Training Time (s)')\n",
    "    for i, v in enumerate(training_times):\n",
    "        axes[1, 0].text(i, v + 0.1, f\"{v:.2f}\", ha='center')\n",
    "    \n",
    "    # Inference time comparison\n",
    "    axes[1, 1].bar(model_names, inference_times)\n",
    "    axes[1, 1].set_title('Inference Time Comparison', fontsize=14)\n",
    "    axes[1, 1].set_ylabel('Inference Time (s)')\n",
    "    for i, v in enumerate(inference_times):\n",
    "        axes[1, 1].text(i, v + 0.001, f\"{v:.4f}\", ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Compile all model metrics for comparison\n",
    "all_models_metrics = {\n",
    "    'SVM': svm_metrics,\n",
    "    'Random Forest': rf_metrics,\n",
    "    'Gradient Boosting': gb_metrics\n",
    "}\n",
    "\n",
    "# Compare all models\n",
    "comparison_df = compare_models(all_models_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b50e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Predictions on Test Data\n",
    "def predict_on_test(model, test_features, class_names, model_name, config):\n",
    "    \"\"\"\n",
    "    Make predictions on test data and create submission file\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        test_features: Test features\n",
    "        class_names: Class names\n",
    "        model_name: Name of the model\n",
    "        config: Configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        predictions: Model predictions\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Making predictions with {model_name} model\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Make predictions\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        predictions_probas = model.predict_proba(test_features)\n",
    "    else:\n",
    "        # For SVM, use decision_function and convert to probabilities\n",
    "        decision_values = model.decision_function(test_features)\n",
    "        # Convert decision values to probabilities using softmax\n",
    "        predictions_probas = softmax(decision_values, axis=1)\n",
    "    \n",
    "    # Inference time\n",
    "    inference_time = (time.time() - start_time) / len(test_features)\n",
    "    print(f\"Average inference time per sample: {inference_time*1000:.2f} ms\")\n",
    "    \n",
    "    # Create submission dataframe\n",
    "    submission_df = pd.DataFrame({\n",
    "        'image_id': test_image_ids,\n",
    "        'healthy': predictions_probas[:, 0],\n",
    "        'multiple_diseases': predictions_probas[:, 1],\n",
    "        'rust': predictions_probas[:, 2],\n",
    "        'scab': predictions_probas[:, 3]\n",
    "    })\n",
    "    \n",
    "    # Save submission file\n",
    "    submission_path = f\"submissions/{model_name.lower().replace(' ', '_')}_submission.csv\"\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    print(f\"Submission file saved to {submission_path}\")\n",
    "    \n",
    "    return predictions_probas\n",
    "\n",
    "# Create submissions directory if it doesn't exist\n",
    "os.makedirs('submissions', exist_ok=True)\n",
    "\n",
    "# For demonstration, let's use the best model based on validation accuracy\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "print(f\"\\nBest model based on validation accuracy: {best_model_name}\")\n",
    "\n",
    "# Get the best model\n",
    "if best_model_name == 'SVM':\n",
    "    best_model = svm_model\n",
    "elif best_model_name == 'Random Forest':\n",
    "    best_model = rf_model\n",
    "elif best_model_name == 'Gradient Boosting':\n",
    "    best_model = gb_model\n",
    "\n",
    "# Make predictions on test data with the best model\n",
    "predictions = predict_on_test(best_model, test_features, config['CLASS_NAMES'], best_model_name, CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc3a009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative Analysis between Traditional ML and Deep Learning Models\n",
    "def compare_ml_vs_dl(ml_model_name, ml_metrics):\n",
    "    \"\"\"\n",
    "    Compare traditional ML models with the deep learning ResNet50 model\n",
    "    \n",
    "    Args:\n",
    "        ml_model_name: Name of the best ML model\n",
    "        ml_metrics: Metrics of the best ML model\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Traditional ML vs Deep Learning Comparison\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Load deep learning model metrics if available\n",
    "    try:\n",
    "        with open('deep_learning_metrics.pkl', 'rb') as f:\n",
    "            dl_metrics = pickle.load(f)\n",
    "            dl_available = True\n",
    "    except:\n",
    "        print(\"Deep learning model metrics not found. Please run the ResNet50 notebook first.\")\n",
    "        dl_available = False\n",
    "    \n",
    "    if not dl_available:\n",
    "        # Create dummy metrics for demonstration purposes\n",
    "        dl_metrics = {\n",
    "            'accuracy': 0.95,  # Example value - replace with actual metrics from ResNet50\n",
    "            'f1_score': 0.94,  # Example value - replace with actual metrics from ResNet50\n",
    "            'training_time': 300,  # Example value - replace with actual metrics from ResNet50\n",
    "            'inference_time': 0.01  # Example value - replace with actual metrics from ResNet50\n",
    "        }\n",
    "        print(\"\\nUsing sample deep learning metrics for demonstration purposes.\")\n",
    "        print(\"For accurate comparison, please run the ResNet50 notebook and save its metrics.\")\n",
    "    \n",
    "    # Create comparison dataframe\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Model': [ml_model_name, 'ResNet50 (Deep Learning)'],\n",
    "        'Accuracy': [ml_metrics['accuracy'], dl_metrics['accuracy']],\n",
    "        'F1 Score': [ml_metrics['f1_score'], dl_metrics['f1_score']],\n",
    "        'Training Time (s)': [ml_metrics['training_time'], dl_metrics['training_time']],\n",
    "        'Inference Time (s)': [ml_metrics['inference_time'], dl_metrics['inference_time']]\n",
    "    })\n",
    "    \n",
    "    # Display comparison table\n",
    "    print(\"\\nML vs DL Comparison Table:\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    models = comparison_df['Model'].tolist()\n",
    "    metrics = ['Accuracy', 'F1 Score', 'Training Time (s)', 'Inference Time (s)']\n",
    "    \n",
    "    # Plot each metric\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i//2, i%2]\n",
    "        values = comparison_df[metric].tolist()\n",
    "        \n",
    "        # Different color for each model\n",
    "        colors = ['#3498db', '#e74c3c']  # Blue for ML, Red for DL\n",
    "        bars = ax.bar(models, values, color=colors)\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for bar, value in zip(bars, values):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{value:.4f}' if value < 0.1 else f'{value:.2f}',\n",
    "                   ha='center', va='bottom', fontsize=12)\n",
    "        \n",
    "        ax.set_title(metric, fontsize=14)\n",
    "        ax.set_ylabel(metric)\n",
    "        \n",
    "        # Set y-axis to start from 0 for accuracy and f1-score\n",
    "        if metric in ['Accuracy', 'F1 Score']:\n",
    "            ax.set_ylim([0, 1])\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate insights\n",
    "    print(\"\\nComparative Insights:\")\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    acc_diff = abs(ml_metrics['accuracy'] - dl_metrics['accuracy'])\n",
    "    if ml_metrics['accuracy'] > dl_metrics['accuracy']:\n",
    "        print(f\"- {ml_model_name} achieves {acc_diff*100:.2f}% higher accuracy than ResNet50\")\n",
    "    else:\n",
    "        print(f\"- ResNet50 achieves {acc_diff*100:.2f}% higher accuracy than {ml_model_name}\")\n",
    "    \n",
    "    # Training time comparison\n",
    "    time_ratio = dl_metrics['training_time'] / ml_metrics['training_time']\n",
    "    print(f\"- ResNet50 takes approximately {time_ratio:.1f}x longer to train than {ml_model_name}\")\n",
    "    \n",
    "    # Inference time comparison\n",
    "    inf_ratio = dl_metrics['inference_time'] / ml_metrics['inference_time'] if ml_metrics['inference_time'] > 0 else 0\n",
    "    if inf_ratio > 1:\n",
    "        print(f\"- ResNet50 takes {inf_ratio:.1f}x longer for inference compared to {ml_model_name}\")\n",
    "    else:\n",
    "        print(f\"- {ml_model_name} takes {1/inf_ratio:.1f}x longer for inference compared to ResNet50\")\n",
    "    \n",
    "    # Overall recommendation\n",
    "    print(\"\\nRecommendation:\")\n",
    "    if dl_metrics['accuracy'] > ml_metrics['accuracy'] + 0.05:\n",
    "        print(f\"- Use ResNet50 when accuracy is the primary concern and training/inference time is less important\")\n",
    "    elif ml_metrics['accuracy'] > dl_metrics['accuracy'] + 0.05:\n",
    "        print(f\"- Use {ml_model_name} for better accuracy and faster training/inference\")\n",
    "    elif ml_metrics['training_time'] < dl_metrics['training_time'] / 3:\n",
    "        print(f\"- Use {ml_model_name} for similar accuracy with significantly faster training/inference\")\n",
    "    else:\n",
    "        print(f\"- Both models perform similarly; choose based on deployment constraints and resource availability\")\n",
    "\n",
    "# Compare best ML model with deep learning\n",
    "compare_ml_vs_dl(best_model_name, all_models_metrics[best_model_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803f92dd",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we implemented several traditional machine learning models for the Plant Pathology 2020 dataset. We:\n",
    "\n",
    "1. **Extracted features** from plant leaf images including color histograms, texture features, shape descriptors, and HOG features.\n",
    "2. **Trained multiple ML models** including Support Vector Machine (SVM), Random Forest, and Gradient Boosting classifiers.\n",
    "3. **Optimized hyperparameters** using GridSearchCV to find the best model configuration.\n",
    "4. **Evaluated model performance** with various metrics including accuracy, precision, recall, F1 score, and confusion matrices.\n",
    "5. **Compared models** to understand their strengths and weaknesses.\n",
    "6. **Generated predictions** on the test dataset and created submission files.\n",
    "7. **Compared traditional ML approaches** with the deep learning ResNet50 model to understand tradeoffs.\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "- Traditional ML models can achieve competitive results compared to deep learning for this image classification task.\n",
    "- Feature engineering plays a crucial role in the performance of traditional ML models.\n",
    "- SVM, Random Forest, and Gradient Boosting each have their own strengths in terms of accuracy, training time, and interpretability.\n",
    "- For production environments with limited computational resources, traditional ML models may offer a good balance of accuracy and efficiency.\n",
    "\n",
    "### Future Work\n",
    "\n",
    "1. **Feature Engineering Enhancement**:\n",
    "   - Explore more advanced feature extraction techniques like Local Binary Patterns (LBP) and SIFT features\n",
    "   - Apply feature selection methods to reduce dimensionality and improve performance\n",
    "\n",
    "2. **Model Improvement**:\n",
    "   - Try ensemble methods combining multiple traditional ML models\n",
    "   - Experiment with other classifiers like XGBoost and LightGBM\n",
    "\n",
    "3. **Hybrid Approaches**:\n",
    "   - Use deep learning for feature extraction (e.g., using pre-trained CNN as feature extractor) and traditional ML for classification\n",
    "   - Create stacked models combining traditional ML and deep learning predictions\n",
    "\n",
    "4. **Explainability**:\n",
    "   - Develop more advanced visualization techniques for feature importance\n",
    "   - Implement SHAP values for better model interpretability\n",
    "\n",
    "5. **Deployment Optimization**:\n",
    "   - Optimize feature extraction pipeline for production deployment\n",
    "   - Create a lightweight model version for edge devices and mobile applications\n",
    "\n",
    "This notebook complements the ResNet50 deep learning approach by providing alternatives that can be more accessible and interpretable while maintaining reasonable performance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
